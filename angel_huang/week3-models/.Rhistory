library(gridExtra)
#library(plyr)
library(dplyr) # for group by
#library(tidyr)
## Load data from rdata that was generated by 01_make_data in data folder
```{r}
load("../../data/students_df.RData")
#df <- read.csv(file = "../../data/TEST_FILE_WEEK2_V2.csv")
dim(students_df)
str(students_df) # list of each variable
df = students_df
```
load("../../data/students_df.RData")
#df <- read.csv(file = "../../data/TEST_FILE_WEEK2_V2.csv")
dim(students_df)
str(students_df) # list of each variable
df = students_df
# reorder levels of class
df$CLASS_LEVEL_STUDENT = ordered(df$CLASS_LEVEL_STUDENT, levels = c('FR','SO','JU','SR'))
# log transform variables
df$LOG_TRNSFR_UNITS_ENTRY = log(df$TRNSFR_UNITS_ENTRY)
df$LOG_TEST_CREDIT_ENTRY = log(df$TEST_CREDIT_ENTRY)
# Partition the data
partition = df %>%
sdf_partition(train = 0.8, test = 0.2, seed = 2020)
# Partition the data
library()
set.seed(2020)
trainMask = sample.split(df$IS_RETAINED, SplitRatio = .75)
# Partition the data
library(caTools)
set.seed(2020)
trainMask = sample.split(df$IS_RETAINED, SplitRatio = .75)
train = subset(data, trainMask == TRUE)
# Partition the data
library(caTools)
set.seed(2020)
trainMask = sample.split(df$IS_RETAINED, SplitRatio = .75)
train = subset(df, trainMask == TRUE)
test  = subset(df, trainMask == FALSE)
load("../../data/students_df.RData")
#df <- read.csv(file = "../../data/TEST_FILE_WEEK2_V2.csv")
dim(students_df)
str(students_df) # list of each variable
summary(students_df)
df = students_df
# reorder levels of class
df$CLASS_LEVEL_STUDENT = ordered(df$CLASS_LEVEL_STUDENT, levels = c('FR','SO','JU','SR'))
# convert variables to factor
df$MAJ_1_SCHOOL_NAME = factor(df$MAJ_1_SCHOOL_NAME)
df$STU_ORIG_ENROLL_STATUS = factor(df$STU_ORIG_ENROLL_STATUS)
# log transform variables
df$LOG_TRNSFR_UNITS_ENTRY = log(df$TRNSFR_UNITS_ENTRY)
df$LOG_TEST_CREDIT_ENTRY = log(df$TEST_CREDIT_ENTRY)
# engineer new feature
df$CREDIT_HOURS_FAILED = df$CREDIT_HOURS_ATT - df$CREDIT_HOURS_EARNED
# Model survival as a function of several predictors
ml_formula <- formula(IS_RETAINED ~ RESIDENCY_CODE + CLASS_LEVEL_STUDENT + IS_DBL_MAJOR + MAJ_1_SCHOOL_NAME + CREDIT_HOURS_FAILED + CUM_RESIDENT_TERMS_BOT + LOG_TRNSFR_UNITS_ENTRY + LOG_TEST_CREDIT_ENTRY + CREDIT_HOURS_FAILED + CREDIT_HOURS_ATT)
# Train a logistic regression model
ml_log <- ml_logistic_regression(train, ml_formula))
# Model survival as a function of several predictors
ml_formula <- formula(IS_RETAINED ~ RESIDENCY_CODE + CLASS_LEVEL_STUDENT + IS_DBL_MAJOR + MAJ_1_SCHOOL_NAME + CREDIT_HOURS_FAILED + CUM_RESIDENT_TERMS_BOT + LOG_TRNSFR_UNITS_ENTRY + LOG_TEST_CREDIT_ENTRY + CREDIT_HOURS_FAILED + CREDIT_HOURS_ATT)
# Train a logistic regression model
ml_log <- ml_logistic_regression(train, ml_formula)
library(sparklyr)
# Model survival as a function of several predictors
ml_formula = formula(IS_RETAINED ~ RESIDENCY_CODE + CLASS_LEVEL_STUDENT + IS_DBL_MAJOR + MAJ_1_SCHOOL_NAME + CREDIT_HOURS_FAILED + CUM_RESIDENT_TERMS_BOT + LOG_TRNSFR_UNITS_ENTRY + LOG_TEST_CREDIT_ENTRY + CREDIT_HOURS_FAILED + CREDIT_HOURS_ATT)
# Train a logistic regression model
ml_log = glm(ml_formula, family='binomial', data = train)
# Partition the data
library(caTools)
set.seed(2020)
trainMask = sample.split(df$IS_RETAINED, SplitRatio = .75)
train = subset(df, trainMask == TRUE)
test  = subset(df, trainMask == FALSE)
# Model survival as a function of several predictors
ml_formula = formula(IS_RETAINED ~ RESIDENCY_CODE + CLASS_LEVEL_STUDENT + IS_DBL_MAJOR + MAJ_1_SCHOOL_NAME + CREDIT_HOURS_FAILED + CUM_RESIDENT_TERMS_BOT + LOG_TRNSFR_UNITS_ENTRY + LOG_TEST_CREDIT_ENTRY + CREDIT_HOURS_FAILED + CREDIT_HOURS_ATT)
# Train a logistic regression model
ml_log = glm(ml_formula, family='binomial', data = train)
# Model survival as a function of several predictors
ml_formula = formula(IS_RETAINED ~ RESIDENCY_CODE + CLASS_LEVEL_STUDENT + IS_DBL_MAJOR + MAJ_1_SCHOOL_NAME + CREDIT_HOURS_FAILED + CUM_RESIDENT_TERMS_BOT + LOG_TRNSFR_UNITS_ENTRY + LOG_TEST_CREDIT_ENTRY + CREDIT_HOURS_FAILED + CREDIT_HOURS_ATT)
# Train a logistic regression model
ml_log = glm(IS_RETAINED ~ RESIDENCY_CODE + CLASS_LEVEL_STUDENT + IS_DBL_MAJOR + MAJ_1_SCHOOL_NAME + CREDIT_HOURS_FAILED + CUM_RESIDENT_TERMS_BOT + LOG_TRNSFR_UNITS_ENTRY + LOG_TEST_CREDIT_ENTRY + CREDIT_HOURS_FAILED + CREDIT_HOURS_ATT, family='binomial', data = train)
# Model survival as a function of several predictors
ml_formula = formula(IS_RETAINED ~ RESIDENCY_CODE + CLASS_LEVEL_STUDENT + IS_DBL_MAJOR + MAJ_1_SCHOOL_NAME + CREDIT_HOURS_FAILED + CUM_RESIDENT_TERMS_BOT + LOG_TRNSFR_UNITS_ENTRY + LOG_TEST_CREDIT_ENTRY + CREDIT_HOURS_FAILED + CREDIT_HOURS_ATT)
# Train a logistic regression model
ml_log = glm(ml_formula, family=binomial, data = train)
# Train a logistic regression model
ml_log = glm(IS_RETAINED ~ RESIDENCY_CODE, family=binomial, data = train)
# Train a logistic regression model
ml_log = glm(IS_RETAINED ~ RESIDENCY_CODE+ CLASS_LEVEL_STUDENT, family=binomial, data = train)
# Train a logistic regression model
ml_log = glm(IS_RETAINED ~ RESIDENCY_CODE+ CLASS_LEVEL_STUDENT, family=binomial, data = train)
# Train a logistic regression model
ml_log = glm(IS_RETAINED ~ RESIDENCY_CODE+ CLASS_LEVEL_STUDENT+ IS_DBL_MAJOR, family=binomial, data = train)
# Train a logistic regression model
ml_log = glm(IS_RETAINED ~ RESIDENCY_CODE+ CLASS_LEVEL_STUDENT+ IS_DBL_MAJOR+ MAJ_1_SCHOOL_NAME, family=binomial, data = train)
# Train a logistic regression model
ml_log = glm(IS_RETAINED ~ RESIDENCY_CODE+ CLASS_LEVEL_STUDENT+ IS_DBL_MAJOR+ MAJ_1_SCHOOL_NAME+ CREDIT_HOURS_FAILED, family=binomial, data = train)
# Model survival as a function of several predictors
ml_formula = formula(IS_RETAINED ~ RESIDENCY_CODE + CLASS_LEVEL_STUDENT + IS_DBL_MAJOR + MAJ_1_SCHOOL_NAME + CUM_RESIDENT_TERMS_BOT + LOG_TRNSFR_UNITS_ENTRY + LOG_TEST_CREDIT_ENTRY + CREDIT_HOURS_FAILED + CREDIT_HOURS_ATT)
# Train a logistic regression model
ml_log = glm(ml_formula, family=binomial, data = train)
# Train a logistic regression model
ml_log = glm(IS_RETAINED ~ RESIDENCY_CODE + CLASS_LEVEL_STUDENT + IS_DBL_MAJOR + MAJ_1_SCHOOL_NAME + CUM_RESIDENT_TERMS_BOT + LOG_TRNSFR_UNITS_ENTRY, family=binomial, data = train)
# Train a logistic regression model
ml_log = glm(IS_RETAINED ~ RESIDENCY_CODE + CLASS_LEVEL_STUDENT + IS_DBL_MAJOR + MAJ_1_SCHOOL_NAME + CUM_RESIDENT_TERMS_BOT, family=binomial, data = train)
# reorder levels of class
df$CLASS_LEVEL_STUDENT = ordered(df$CLASS_LEVEL_STUDENT, levels = c('FR','SO','JU','SR'))
# convert variables to factor
df$MAJ_1_SCHOOL_NAME = factor(df$MAJ_1_SCHOOL_NAME)
df$STU_ORIG_ENROLL_STATUS = factor(df$STU_ORIG_ENROLL_STATUS)
# log transform variables
df$LOG_TRNSFR_UNITS_ENTRY = log(df$TRNSFR_UNITS_ENTRY+0.1) # add a small number to make sure log is >0
df$LOG_TEST_CREDIT_ENTRY = log(df$TEST_CREDIT_ENTRY+0.1)
# engineer new feature
df$CREDIT_HOURS_FAILED = df$CREDIT_HOURS_ATT - df$CREDIT_HOURS_EARNED
# Model survival as a function of several predictors
ml_formula = formula(IS_RETAINED ~ RESIDENCY_CODE + CLASS_LEVEL_STUDENT + IS_DBL_MAJOR + MAJ_1_SCHOOL_NAME + CUM_RESIDENT_TERMS_BOT + LOG_TRNSFR_UNITS_ENTRY + LOG_TEST_CREDIT_ENTRY + CREDIT_HOURS_FAILED + CREDIT_HOURS_ATT)
# Train a logistic regression model
ml_log = glm(ml_formula, family=binomial, data = train)
# Partition the data
library(caTools)
set.seed(2020)
trainMask = sample.split(df$IS_RETAINED, SplitRatio = .75)
train = subset(df, trainMask == TRUE)
test  = subset(df, trainMask == FALSE)
# Model survival as a function of several predictors
ml_formula = formula(IS_RETAINED ~ RESIDENCY_CODE + CLASS_LEVEL_STUDENT + IS_DBL_MAJOR + MAJ_1_SCHOOL_NAME + CUM_RESIDENT_TERMS_BOT + LOG_TRNSFR_UNITS_ENTRY + LOG_TEST_CREDIT_ENTRY + CREDIT_HOURS_FAILED + CREDIT_HOURS_ATT)
# Train a logistic regression model
ml_log = glm(ml_formula, family=binomial, data = train)
# Summarize the model
summary(ml_log)
# Summarize the model
summary(ml_log)
df = students_df
# convert variables to factor
df$MAJ_1_SCHOOL_NAME = factor(df$MAJ_1_SCHOOL_NAME)
df$STU_ORIG_ENROLL_STATUS = factor(df$STU_ORIG_ENROLL_STATUS)
df$CLASS_LEVEL_STUDENT = factor(df$CLASS_LEVEL_STUDENT)
# reorder levels of class
df$ORDER_CLASS_LEVEL_STUDENT = ordered(df$CLASS_LEVEL_STUDENT, levels = c('FR','SO','JU','SR'))
# log transform variables
df$LOG_TRNSFR_UNITS_ENTRY = log(df$TRNSFR_UNITS_ENTRY+0.1) # add a small number to make sure log is not inf
df$LOG_TEST_CREDIT_ENTRY = log(df$TEST_CREDIT_ENTRY+0.1)
# engineer new feature
df$CREDIT_HOURS_FAILED = df$CREDIT_HOURS_ATT - df$CREDIT_HOURS_EARNED
# Partition the data
library(caTools)
set.seed(2020)
trainMask = sample.split(df$IS_RETAINED, SplitRatio = .75)
train = subset(df, trainMask == TRUE)
test  = subset(df, trainMask == FALSE)
# Model survival as a function of several predictors
ml_formula1 = formula(IS_RETAINED ~ RESIDENCY_CODE + ORDER_CLASS_LEVEL_STUDENT + IS_DBL_MAJOR + MAJ_1_SCHOOL_NAME + CUM_RESIDENT_TERMS_BOT + LOG_TRNSFR_UNITS_ENTRY + LOG_TEST_CREDIT_ENTRY + CREDIT_HOURS_FAILED + CREDIT_HOURS_ATT)
ml_formula2 = formula(IS_RETAINED ~ RESIDENCY_CODE + CLASS_LEVEL_STUDENT + IS_DBL_MAJOR + MAJ_1_SCHOOL_NAME + CUM_RESIDENT_TERMS_BOT + LOG_TRNSFR_UNITS_ENTRY + LOG_TEST_CREDIT_ENTRY + CREDIT_HOURS_FAILED + CREDIT_HOURS_ATT)
# Train a logistic regression model
ml_log1 = glm(ml_formula1, family=binomial, data = train)
ml_log2 = glm(ml_formula2, family=binomial, data = train)
# Summarize the model
summary(ml_log1)
summary(ml_log2)
exp(1.6)
exp(0.6)
#Predict using the model
x = test[,-c("IS_RETAINED","IS_RETAINED_DESC_"]
#Predict using the model
x = test[,-c("IS_RETAINED","IS_RETAINED_DESC_")]
#Predict using the model
x = test[,-c("IS_RETAINED","IS_RETAINED_DESC")]
#Predict using the model
x = subset(test,selext=c("IS_RETAINED","IS_RETAINED_DESC"))
y = test$IS_RETAINED
probability = predict(ml_formula2,x,type="response")
#Predict using the model
x = subset(test,selext=c("IS_RETAINED","IS_RETAINED_DESC"))
y = test$IS_RETAINED
probability = predict(ml_log2,x,type="response")
test$pred_log_reg=apply(probability,1,which.max)
optCutOff
#Predict using the model
x = subset(test,selext=c("IS_RETAINED","IS_RETAINED_DESC"))
y = test$IS_RETAINED
test$lreg_prob = predict(ml_log2,x,type="response")
library(InformationValue)
install.packages("InformationValue")
#Predict using the model
x = subset(test,selext=c("IS_RETAINED","IS_RETAINED_DESC"))
y = test$IS_RETAINED
test$lreg_prob = predict(ml_log2,x,type="response")
library(InformationValue)
optCutOff = optimalCutoff(test$IS_RETAINED, test$lreg_prob)[1]
optCutOff
misClassError(test$IS_RETAINED, test$lreg_prob, threshold = optCutOff)
#Accuracy of the model
mtab = table(test$pred_log_reg,test$IS_RETAINED)
#Accuracy of the model
mtab = table(test$lreg_prob,test$IS_RETAINED)
library(caret)
install.packages("caret")
library(caret)
library(caret)
confusionMatrix(mtab)
test$lreg_pred = test$lreg_prob >= optCutOff
sum(test$lreg_pred)
dim(test)
#Accuracy of the model
mtab = table(test$lreg_pred,test$IS_RETAINED)
library(caret)
confusionMatrix(mtab)
test$lreg_pred = as.numeric(test$lreg_prob >= optCutOff)
sum(test$lreg_pred) # =3535
dim(test) # 3560 x 43
misClassError(test$IS_RETAINED, test$lreg_prob, threshold = optCutOff)
#Accuracy of the model
mtab = table(test$lreg_pred,test$IS_RETAINED)
library(caret)
confusionMatrix(mtab)
plotROC(testData$IS_RETAINED, test$lreg_prob)
plotROC(test$IS_RETAINED, test$lreg_prob)
test$lreg_pred = as.numeric(test$lreg_prob >= optCutOff)
sum(test$lreg_pred) # =3535
dim(test) # 3560 x 43
misClassError(test$IS_RETAINED, test$lreg_prob, threshold = optCutOff)
# 0.0295
#Accuracy of the model
mtab = table(test$lreg_pred,test$IS_RETAINED)
library(caret)
confusionMatrix(mtab)
plotROC(test$IS_RETAINED, test$lreg_prob)
test$lreg_pred = as.numeric(test$lreg_prob >= optCutOff)
sum(test$lreg_pred) # =3535
dim(test) # 3560 x 43
misClassError(test$IS_RETAINED, test$lreg_prob, threshold = optCutOff)
# 0.0295
#Accuracy of the model
mtab = table(test$IS_RETAINED,test$lreg_pred)
library(caret)
confusionMatrix(mtab)
plotROC(test$IS_RETAINED, test$lreg_prob)
#Summarize the model
summary(ml_lda)
library(MASS)
#Build the model
ml_lda<-lda(ml_formula2,data=df)
#Summarize the model
summary(ml_lda)
library(MASS)
#Build the model
ml_lda<-lda(ml_formula2,data=train)
#Summarize the model
summary(ml_lda)
#Predict using the model and find best cutoff
x = subset(test,selext=c("IS_RETAINED","IS_RETAINED_DESC"))
y = test$IS_RETAINED
test$prob_lreg = predict(ml_log2,x,type="response")
library(InformationValue)
optCutOff = optimalCutoff(test$IS_RETAINED, test$prob_lreg)[1]
optCutOff #=0.63
test$pred_lreg = as.numeric(test$prob_lreg >= optCutOff)
sum(test$pred_lreg) # =3535
dim(test) # 3560 x 43
misClassError(test$IS_RETAINED, test$lreg_prob, threshold = optCutOff)
# 0.0295
#Accuracy of the model
mtab = table(test$IS_RETAINED,test$pred_lreg)
library(caret)
confusionMatrix(mtab)
plotROC(test$IS_RETAINED, test$prob_lreg)
library(MASS)
#Build the model
ml_lda<-lda(ml_formula2,data=train)
#Summarize the model
summary(ml_lda)
#Predict using the model
test$pred_lda = predict(ml_lda,x)$class
#Accuracy of the model
mtab = table(test$IS_RETAINED,test_pred_lda)
#Predict using the model
test$pred_lda = predict(ml_lda,x)$class
#Accuracy of the model
mtab = table(test$IS_RETAINED,test$pred_lda)
confusionMatrix(mtab)
confusionMatrix(mtab)
plotROC(test$IS_RETAINED, test$pred_lda)
plotROC(test$IS_RETAINED, as.numeric(test$pred_lda))
library(mda)
install.packages("mda")
library(mda)
#Build the model
ml_mda = mda(ml_formula2,data=train)
#Summarize the model
summary(ml_mda)
#Predict using the model
test$pred_mda = predict(ml_mda,x)
#Accuracy of the model
mtab = table(test$IS_RETAINED,test$pred_mda)
confusionMatrix(mtab)
confusionMatrix(mtab)
#Predict using the model
test$pred_mda = predict(ml_mda,x)
#Accuracy of the model
mtab = table(test$IS_RETAINED,test$pred_mda)
confusionMatrix(mtab)
library(kernlab)
install.packages("kernlab")
library(kernlab)
ml_svm = ksvm(ml_formula2,data=train)
#Summarize the model
summary(ml_mda)
#Summarize the model
summary(ml_svm)
#Predict using the model
test$pred_svm = predict(ml_svm,x)
#Accuracy of the model
mtab = table(test$IS_RETAINED,test$pred_svm)
confusionMatrix(mtab)
#Predict using the model
test$pred_svm = predict(ml_svm,x,type='response')
optCutOff
#Predict using the model
test$prob_svm = predict(ml_svm,x,type='response')
optCutOff = optimalCutoff(test$IS_RETAINED, test$prob_svm)[1]
optCutOff
test$pred_svm = as.numeric(test$prob_svm>=optCutOff)
#Accuracy of the model
mtab = table(test$IS_RETAINED,test$pred_svm)
confusionMatrix(mtab)
optCutOff # 0.8
#Predict using the model
test$prob_svm = predict(ml_svm,x,type='response')
optCutOff = optimalCutoff(test$IS_RETAINED, test$prob_svm)[1]
optCutOff # 0.86
test$pred_svm = as.numeric(test$prob_svm>=optCutOff)
#Accuracy of the model
mtab = table(test$IS_RETAINED,test$pred_svm)
confusionMatrix(mtab)
plotROC(test$IS_RETAINED, test$prob_lreg)
plotROC(test$IS_RETAINED, test$prob_svm)
install.packages("rpart")
library(rpart)
#Build the model
# tree doesn't need log transform
ml_formula3 = formula(IS_RETAINED ~ RESIDENCY_CODE + CLASS_LEVEL_STUDENT + IS_DBL_MAJOR + MAJ_1_SCHOOL_NAME + CUM_RESIDENT_TERMS_BOT + TRNSFR_UNITS_ENTRY + TEST_CREDIT_ENTRY + CREDIT_HOURS_FAILED + CREDIT_HOURS_ATT)
ml_cart = rpart(ml_formula3,data=df)
#Summarize the model
summary(ml_cart)
library(rpart.plot)
library(rpart)
library(rpart.plot)
# Visualize the decision tree with rpart.plot
rpart.plot(ml_tree, box.palette="RdBu", shadow.col="gray", nn=TRUE)
install.packages("rpart.plot")
library(rpart.plot)
# Visualize the decision tree with rpart.plot
rpart.plot(ml_tree, box.palette="RdBu", shadow.col="gray", nn=TRUE)
library(rpart)
library(rpart.plot)
#Build the model
# tree doesn't need log transform
ml_formula3 = formula(IS_RETAINED ~ RESIDENCY_CODE + CLASS_LEVEL_STUDENT + IS_DBL_MAJOR + MAJ_1_SCHOOL_NAME + CUM_RESIDENT_TERMS_BOT + TRNSFR_UNITS_ENTRY + TEST_CREDIT_ENTRY + CREDIT_HOURS_FAILED + CREDIT_HOURS_ATT)
ml_tree = rpart(ml_formula3,data=df)
#Summarize the model
summary(ml_tree)
# Visualize the decision tree with rpart.plot
rpart.plot(ml_tree, box.palette="RdBu", shadow.col="gray", nn=TRUE)
help rpart
doc rpart
#Predict using the model
test$pred_tree = predict(ml_tree,x,type="class")
#Predict using the model
test$pred_tree = predict(ml_tree,x,type="response")
#Predict using the model
test$pred_tree = predict(ml_tree,x,type="class")
df$FAC_IS_RETAINED = factor(df$IS_RETAINED)
trainMask = sample.split(df$IS_RETAINED, SplitRatio = .75)
# Partition the data
library(caTools)
set.seed(2020)
trainMask = sample.split(df$IS_RETAINED, SplitRatio = .75)
train = subset(df, trainMask == TRUE)
test  = subset(df, trainMask == FALSE)
library(rpart)
library(rpart.plot)
#Build the model
# For classification tree, response needs to be factor
# tree doesn't need log transform
ml_formula3 = formula(FAC_IS_RETAINED ~ RESIDENCY_CODE + CLASS_LEVEL_STUDENT + IS_DBL_MAJOR + MAJ_1_SCHOOL_NAME + CUM_RESIDENT_TERMS_BOT + TRNSFR_UNITS_ENTRY + TEST_CREDIT_ENTRY + CREDIT_HOURS_FAILED + CREDIT_HOURS_ATT)
ml_tree = rpart(ml_formula3,data=df)
#Summarize the model
summary(ml_tree)
# Visualize the decision tree with rpart.plot
rpart.plot(ml_tree, box.palette="RdBu", shadow.col="gray", nn=TRUE)
#Predict using the model
test$pred_tree = predict(ml_tree,x,type="class")
#Predict using the model
test$pred_tree = predict(ml_tree,x,type="class")
#Accuracy of the model
mtab = table(test$IS_RETAINED,test$pred_tree)
confusionMatrix(mtab)
ml_tree = rpart(ml_formula3,data=df,cp=0.02) # cp for complexity, smaller cp =more branches
#Summarize the model
summary(ml_tree)
# Visualize the decision tree with rpart.plot
rpart.plot(ml_tree, box.palette="RdBu", shadow.col="gray", nn=TRUE)
ml_tree = rpart(ml_formula3,data=df,cp=0.01) # cp for complexity, smaller cp =more branches
#Summarize the model
summary(ml_tree)
#Summarize the model
summary(ml_tree)
# Visualize the decision tree with rpart.plot
rpart.plot(ml_tree, box.palette="RdBu", shadow.col="gray", nn=TRUE)
library(randomForest)
install.packages("randomForest")
library(randomForest)
#Build the model
ml_forest = randomForest(ml_formula3,data=df)
# convert variables to factor
df$RESIDENCY_CODE = factor(df$RESIDENCY_CODE)
train = subset(df, trainMask == TRUE)
test  = subset(df, trainMask == FALSE)
#Build the model (need to make sure no chr variables, convert to factors)
ml_forest = randomForest(ml_formula3,data=df)
library(randomForest)
#Build the model (need to make sure no chr variables, convert to factors)
ml_forest = randomForest(ml_formula3,data=df)
#Summarize the model
summary(ml_forest)
getTree(ml_forest, 1) # check an example tree
importance(ml_forest) # list of feature importance
library(randomForest)
#Build the model (need to make sure no chr variables, convert to factors)
ml_rf = randomForest(ml_formula3,data=df)
#Summarize the model
summary(ml_rf)
#getTree(ml_rf, 1) # check an example tree
importance(ml_rf) # list of feature importance
library(randomForestExplainer)
install.packages("randomForestExplainer")
install.packages("randomForestExplainer")
library(randomForestExplainer)
#setwd(my/destination/path)
explain_forest(ml_rf, interactions = TRUE, data = train)
library(randomForest)
#Build the model (need to make sure no chr variables, convert to factors)
ml_rf = randomForest(ml_formula3,data=df,localImp = TRUE) # add local importance interpretation for later
#Summarize the model
summary(ml_rf)
#getTree(ml_rf, 1) # check an example tree
importance(ml_rf) # list of feature importance
#setwd(my/destination/path)
explain_forest(ml_rf, interactions = TRUE, data = train)
install.packages("gbm")
install.packages("xgboost")
#Predict using the model
test$pred_rf = predict(ml_rf,x,type="class")
#Predict using the model
test$pred_rf = predict(ml_rf,x)
#Predict using the model
xtest = cbind(train[1,],x) # to equalize classes of training and test set
xtest = xtest[-1,] # delete it
test$pred_rf = predict(ml_rf,xtest)
#Predict using the model
xtest = cbind(train[1,],x) # to equalize classes of training and test set
test$pred_rf = predict(ml_rf,xtest)
#Accuracy of the model
mtab = table(test$IS_RETAINED,test$pred_rf)
confusionMatrix(mtab)
library(gbm)
#Build the model
ml_gbm = gbm(ml_formula3,data=train,distribution="binomial")
#Build the model
ml_gbm = gbm(ml_formula3,data=train,distribution="multinomial")
#Summarize the model
summary(ml_gbm)
#Predict using the model
test$prob_gbm = predict(ml_gbm,x,n.trees=1)
optCutOff = optimalCutoff(test$IS_RETAINED, test$prob_gbm)[1]
optCutOff # 0.86
test$pred_gbm = as.numeric(test$prob_gbm>=optCutOff)
optCutOff # 0.86
library(xgboost)
#Build the model
trainXGB = sparse.model.matrix(ml_formula3, data = train) # generate sparse encoding data in a list
require(Matrix)
#Build the model
trainXGB = sparse.model.matrix(ml_formula3, data = train) # generate sparse encoding data in a list
ml_xgb = xgboost(ml_formula3,data=trainXGB, label = train$IS_RETAINED,max.depth=3,eta=0.3,nround=5,seed = 1,objective = "binary:logistic")
#Summarize the model
summary(ml_xgb)
#Predict using the model
test$pred_xgb = predict(ml_xgb,x)
testXGB = sparse.model.matrix(ml_formula3, data = x) # generate sparse encoding data in a list
testXGB = sparse.model.matrix(ml_formula3, data = test) # generate sparse encoding data in a list
#Predict using the model
test$pred_xgb = predict(ml_xgb,testXGB)
#Predict using the model
test$pred_xgb = predict(ml_xgb,testXGB)
#Accuracy of the model
mtab = table(test$IS_RETAINED,test$pred_xgb)
confusionMatrix(mtab)
optCutOff = optimalCutoff(test$IS_RETAINED, test$pred_xgb)[1]
optCutOff # -0.18 something is wrong...
#Predict using the model
test$prob_xgb = predict(ml_xgb,testXGB)
optCutOff = optimalCutoff(test$IS_RETAINED, test$pred_xgb)[1]
optCutOff # 0.64
test$pred_xgb = as.numeric(test$prob_xgb>=optCutOff)
#Accuracy of the model
mtab = table(test$IS_RETAINED,test$pred_xgb)
confusionMatrix(mtab)
plotROC(test$IS_RETAINED, test$prob_xgb)
