---
title: "03_model_regression_tree"
author: "Angel Huang"
date: "10/29/2020"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# 1 Prepare data
## Load libraries
```{r}
# Install EDA related libraries
library(tidyverse)
library(ggplot2)
library(gridExtra)
#library(plyr)
library(dplyr) # for group by
#library(tidyr)
```

## Load data from rdata that was generated by 01_make_data in data folder
```{r}
load("../../data/students_df.RData")
#df <- read.csv(file = "../../data/TEST_FILE_WEEK2_V2.csv")
dim(students_df)
str(students_df) # list of each variable
summary(students_df)
df = students_df
```

## Transformation of variables
```{r}
# convert variables to factor
df$RESIDENCY_CODE = factor(df$RESIDENCY_CODE)
df$CLASS_LEVEL_STUDENT = factor(df$CLASS_LEVEL_STUDENT)
df$MAJ_1_SCHOOL_NAME = factor(df$MAJ_1_SCHOOL_NAME)
df$STU_ORIG_ENROLL_STATUS = factor(df$STU_ORIG_ENROLL_STATUS)
df$FAC_IS_RETAINED = factor(df$IS_RETAINED)
# reorder levels of class
df$ORDER_CLASS_LEVEL_STUDENT = ordered(df$CLASS_LEVEL_STUDENT, levels = c('FR','SO','JU','SR'))

# log transform variables
df$LOG_TRNSFR_UNITS_ENTRY = log(df$TRNSFR_UNITS_ENTRY+0.1) # add a small number to make sure log is not inf
df$LOG_TEST_CREDIT_ENTRY = log(df$TEST_CREDIT_ENTRY+0.1)
# engineer new feature
df$CREDIT_HOURS_FAILED = df$CREDIT_HOURS_ATT - df$CREDIT_HOURS_EARNED

```
I reordered the class level so that it makes more sense -- from low to high grade. Since some variables are skewed, I created log transform of those variables for linear models later.

# 2 Modeling
## Train-validation split
Randomly partition the data into train and test sets.
```{r partition}
# Partition the data
library(caTools)
set.seed(2020) 
trainMask = sample.split(df$IS_RETAINED, SplitRatio = .75)
train = subset(df, trainMask == TRUE)
test  = subset(df, trainMask == FALSE)

```

# Train models on training data
## 2.1 Logistic regression
```{r train}
# Model survival as a function of several predictors
ml_formula1 = formula(IS_RETAINED ~ RESIDENCY_CODE + ORDER_CLASS_LEVEL_STUDENT + IS_DBL_MAJOR + MAJ_1_SCHOOL_NAME + CUM_RESIDENT_TERMS_BOT + LOG_TRNSFR_UNITS_ENTRY + LOG_TEST_CREDIT_ENTRY + CREDIT_HOURS_FAILED + CREDIT_HOURS_ATT)

ml_formula2 = formula(IS_RETAINED ~ RESIDENCY_CODE + CLASS_LEVEL_STUDENT + IS_DBL_MAJOR + MAJ_1_SCHOOL_NAME + CUM_RESIDENT_TERMS_BOT + LOG_TRNSFR_UNITS_ENTRY + LOG_TEST_CREDIT_ENTRY + CREDIT_HOURS_FAILED + CREDIT_HOURS_ATT)
# Train a logistic regression model
ml_log1 = glm(ml_formula1, family=binomial, data = train)
ml_log2 = glm(ml_formula2, family=binomial, data = train)
# Summarize the model
summary(ml_log1)
summary(ml_log2)
#confint(ml_log2) # to get confidence interval
```
```{r}
exp(1.6)
exp(0.6)
```

Note that for ordered factors, by default, R fits a series of polynomial functions to the levels of the variable. The first is linear (.L), the second is quadratic (.Q), the third (if you had enough levels) would be cubic, etc. R will fit one fewer polynomial function than the number of levels in your variable. For example, if you have only two levels, only the linear trend would be fit. Moreover, the polynomial bases used are orthogonal.

In logistic regression model, significant and positive variables are: 
* CLASS_LEVEL_STUDENT (linear) -- higher level students are more likely to return. In addition, I also fit the model with un-ordered ClASS_LEVEL_STUDENT and it appears that freshman is the mostly likely to drop. Especially junior and seniors have larger odds of return against freshman (~e^1.6 = 4.95, i.e. the odds for junior and senior to return are about 4 times higher than freshman) compared to sophermore (~e^0.6 = 1.82, i.e. the odds for sophermore to return is about 82% higher than freshman). 
* IS_DBL_MAJOR -- double major students are more likely to be retained. (maybe b/c of higher self-expectation)
* MAJ_1_SCHOOL -- only business school students are more likely to be retained. (interesting)
* LOG_TEST_CREDIT_ENTRY -- higher test credit students are more likely to return. (past performance is a good indicator of future performance)
* CREDIT_HOURS_ATT -- students who attempted more credits are more likely to return. (higher self-expectation)

Significant and negative variables are: 
* RESIDENTIAL_CODEO -- Out-of-state students are more likely to drop.
* CREDIT_HOURS_FAILED -- Students who failed more credits this semester are more likely to drop, which makes sense. (This is also where we can intervene)
* CUM_RESIDENT_TERMS_BOT -- After controlling for class level, students who have longer cumulative resident term are more likely to drop. (This is puzzling, is it because of the students who have stayed more semester than needed?)


```{r test}
#Predict using the model and find best cutoff
x = subset(test,selext=c("IS_RETAINED","IS_RETAINED_DESC"))
y = test$IS_RETAINED
test$prob_lreg = predict(ml_log2,x,type="response")
library(InformationValue)
optCutOff = optimalCutoff(test$IS_RETAINED, test$prob_lreg)[1] 
optCutOff #=0.63
```
The best cutoff is 0.63.

```{r}
test$pred_lreg = as.numeric(test$prob_lreg >= optCutOff)
sum(test$pred_lreg) # =3535
dim(test) # 3560 x 43 
misClassError(test$IS_RETAINED, test$lreg_prob, threshold = optCutOff)
# 0.0295

#Accuracy of the model
mtab = table(test$IS_RETAINED,test$pred_lreg)
library(caret)
confusionMatrix(mtab)
plotROC(test$IS_RETAINED, test$prob_lreg)
```
Logistic regression result: The best cutoff to maximize test accuracy is 0.63, which results in 3535 out of 3560 students classified as IS_RETAINED. Using this threshold, we assign students with probability >= 0.63 as returned, and others as dropped. This gives us accuracy = 0.97, sensitivity = 0.16 (pretty low), specificity = 0.998 (very high). The confusion matrix shows that the classifier is doing very well when real class is 1, but not as good when real class is 0 (detected 18 out of 18+98=116 negative cases). The area under the ROC curve is 0.7842, which is ok.


## 2.2 Linear Discriminant Analysis
Discriminant function analysis is highly sensitive to outliers. Each group should have the same variance for any independent variable (that is, be homoscedastic), although the variances can differ among the independent variables. For many types of data, a log transformation will make the data more homoscedastic.
```{r train}
library(MASS)
#Build the model
ml_lda<-lda(ml_formula2,data=train)

#Summarize the model
summary(ml_lda)

```
```{r test}
#Predict using the model
test$pred_lda = predict(ml_lda,x)$class

#Accuracy of the model
mtab = table(test$IS_RETAINED,test$pred_lda)
confusionMatrix(mtab)
```
LDA result: accuracy is 0.94 (lower than logistic regression). Has higher sensitivity (0.21) and lower specificity (0.98).

## 2.3 Mixture Discriminant Analysis (non-linear)
```{r train}
library(mda)

#Build the model
ml_mda = mda(ml_formula2,data=train)

#Summarize the model
summary(ml_mda)
```
```{r test}
#Predict using the model
test$pred_mda = predict(ml_mda,x)

#Accuracy of the model
mtab = table(test$IS_RETAINED,test$pred_mda)
confusionMatrix(mtab)
```

MDA result: accuracy 0.95, sensitivity 0.28, specificity 0.97.

## 2.4 Support Vector Machine
```{r train}
library(kernlab)
ml_svm = ksvm(ml_formula2,data=train)
#Summarize the model
summary(ml_svm)
```

```{r test}
#Predict using the model
test$prob_svm = predict(ml_svm,x,type='response')
optCutOff = optimalCutoff(test$IS_RETAINED, test$prob_svm)[1]
optCutOff # 0.86
test$pred_svm = as.numeric(test$prob_svm>=optCutOff)

#Accuracy of the model
mtab = table(test$IS_RETAINED,test$pred_svm)
confusionMatrix(mtab)
plotROC(test$IS_RETAINED, test$prob_svm)
```
SVM result: accuracy = 0.97, sensitivity 0.67 (highest so far), specificity 0.97. Area under the ROC curve is 0.64, kinda low.

## 2.5 Classification and Regression Tree (CART)
```{r train}
library(rpart)
library(rpart.plot)
#Build the model
# For classification tree, response needs to be factor
# tree doesn't need log transform
ml_formula3 = formula(FAC_IS_RETAINED ~ RESIDENCY_CODE + CLASS_LEVEL_STUDENT + IS_DBL_MAJOR + MAJ_1_SCHOOL_NAME + CUM_RESIDENT_TERMS_BOT + TRNSFR_UNITS_ENTRY + TEST_CREDIT_ENTRY + CREDIT_HOURS_FAILED + CREDIT_HOURS_ATT)

ml_tree = rpart(ml_formula3,data=df,cp=0.01) # cp for complexity, smaller cp =more branches

#Summarize the model
summary(ml_tree)
# Visualize the decision tree with rpart.plot
rpart.plot(ml_tree, box.palette="RdBu", shadow.col="gray", nn=TRUE)
```
Tree model: blue = 1 = returned. First split is if credit_hours_failed >= 13, 1% students fall into this category, and the probablity of returning is 52%. Then if credit hours attempted < 16, the probability of student returning is lower (47%). The student most unlikely to return (reddest block) is those with less than 5.5 attempted hours and fail more than 2.
It seems like the most important 3 factors are CREDIT_HOURS_FAILED, CREDIT_HOURS_ATT, and TRANSFER_UNITS_ENTRY. More credit hours attempted, less failed, and more transfer units are related to high chance of return. This makes sense.

```{r test}
#Predict using the model
test$pred_tree = predict(ml_tree,x,type="class") 

#Accuracy of the model
mtab = table(test$IS_RETAINED,test$pred_tree)
confusionMatrix(mtab)

```

Tree result: accuracy is 0.97, sensitivity 0.65, specificity 0.97. Best result so far. Also good for interpretation of the model.

## 2.6 RandomForest
```{r train}
library(randomForest)

#Build the model (need to make sure no chr variables, convert to factors)
ml_rf = randomForest(ml_formula3,data=df,localImp = TRUE) # add local importance interpretation for later

#Summarize the model
summary(ml_rf)
#getTree(ml_rf, 1) # check an example tree
importance(ml_rf) # list of feature importance
```

Random Forest Model: feature importance rank: CREDIT_HOURS_FAILED > CREDIT_HOURS_ATT > CUM_RESIDENT_TERMS_BOT > TEST_CREDIT_ENTRY > TRANSFOR_UNITS_ENTRY > CLASS_LEVEL_STUDENT  > RESIDENCY_CODE > IS_DBL_MAJOR

# Here is some plot for random forest interpretation
```{r}
# Takes too long, only run if necessary 
# library(randomForestExplainer)
# #setwd(my/destination/path)
# explain_forest(ml_rf, interactions = TRUE, data = train)
# # This will generate an .html file, named Your_forest_explained.html, in your my/destination/path that you can easily open in a Web Browser.
# # In this report you'll find useful information about the structure of trees and forest and several useful statistics about the variables.
```
Note that this explain_forest takes a long time to run. (>1h)

# Test
```{r test}
#Predict using the model
xtest = cbind(train[1,],x) # to equalize classes of training and test set, otherwise throw error
test$pred_rf = predict(ml_rf,xtest) 

#Accuracy of the model
mtab = table(test$IS_RETAINED,test$pred_rf)
confusionMatrix(mtab)

```
Random Forest Result: all classified as returned, not a useful model.

## 2.7 Gradient Boost Machine
```{r train}
library(gbm)

#Build the model
ml_gbm = gbm(ml_formula3,data=train,distribution="multinomial") # binomial is not supported (why?)

#Summarize the model
summary(ml_gbm)
```

Note: Gradient Boosting Machines vs. XGBoost. ... While regular gradient boosting uses the loss function of our base model (e.g. decision tree) as a proxy for minimizing the error of the overall model, XGBoost uses the 2nd order derivative as an approximation.

From the relative influence table, we can see that CREDIT_HOURS_FAILED > CREDIT_HOURS_ATT > CUM_RESIDENT_TERMS_BOT. All other variables have relative influence less than 1.

# Test
```{r test}
#Predict using the model
test$prob_gbm = predict(ml_gbm,x,n.trees=1) 
optCutOff = optimalCutoff(test$IS_RETAINED, test$prob_gbm)[1]
optCutOff # -0.18 something is wrong...
test$pred_gbm = as.numeric(test$prob_gbm>=optCutOff)

#Accuracy of the model
mtab = table(test$IS_RETAINED,test$pred_gbm)
confusionMatrix(mtab)
plotROC(test$IS_RETAINED, test$prob_gbm)
```
Need to debug, prob_gbm are all -0.18.


## 2.8 Extreme Gradient Boost Machine (XGBoost)
```{r train}
library(xgboost)
library(Matrix)
#Build the model
trainXGB = sparse.model.matrix(ml_formula3, data = train) # generate sparse encoding data in a list
testXGB = sparse.model.matrix(ml_formula3, data = test) # generate sparse encoding data in a list

ml_xgb = xgboost(ml_formula3,data=trainXGB, label = train$IS_RETAINED,max.depth=3,eta=0.3,nround=5,seed = 1,objective = "binary:logistic")
# max.depth = 3: the trees wonâ€™t be deep, because our case is very simple ;
# eta = 1: learning rate, range [0,1], default = 0.3
# nthread = 2: the number of cpu threads we are going to use;
# nrounds = 2: there will be two passes on the data, the second one will enhance the model by further reducing the difference between ground truth and prediction.

#Summarize the model
summary(ml_xgb)
```

XGBoost only works with numeric vectors, so need to convert all categorical variable into numeric using one hot encoding. Sparse Matrix is a matrix where most of the values of zeros. Conversely, a dense matrix is a matrix where most of the values are non-zeros.
Can't really see what is in the model.

# Test
```{r test}
#Predict using the model
test$prob_xgb = predict(ml_xgb,testXGB) 
optCutOff = optimalCutoff(test$IS_RETAINED, test$pred_xgb)[1]
optCutOff # 0.64
test$pred_xgb = as.numeric(test$prob_xgb>=optCutOff)

#Accuracy of the model
mtab = table(test$IS_RETAINED,test$pred_xgb)
confusionMatrix(mtab)
plotROC(test$IS_RETAINED, test$prob_xgb)
```

XGBoost result: Best cutoff is 0.64, accuracy is 0.97, sensitivity is 0.56, specificity is 0.97. Area under ROC is 0.766.
Compared to tree result: accuracy is 0.97, sensitivity 0.65, specificity 0.97. Still tree is better.

