mod_df,
col=c("lightgoldenrod", "lightcyan"),
ylab = "Retention", xlab ="Credits Earned", main = "Conditional density plot")
fit2 <- glm(IS_RETAINED ~ CLASS_LEVEL_STUDENT*CREDIT_HOURS_EARNED,
data = mod_df,
family = binomial(link = "logit"))
summary(fit2)
anova(fit1, fit2, test = "LR")
fit2a <- glm(IS_RETAINED ~ CLASS_LEVEL_STUDENT + CREDIT_HOURS_EARNED,
data = mod_df,
family = binomial(link = "logit"))
summary(fit2a)
anova(fit2a, fit2, test = "LR")
fit2_emtrends <- emtrends(fit2, "CLASS_LEVEL_STUDENT",
var = "CREDIT_HOURS_EARNED",
transform = "response")
fit2_emtrends
plot(fit2_emtrends)
mod2_df <- mod_df %>%
mutate(CREDITS_FAILED = CREDIT_HOURS_ATT - CREDIT_HOURS_EARNED)
cdplot(factor(IS_RETAINED) ~ CREDITS_FAILED,
mod2_df,
col=c("lightgoldenrod", "lightcyan"),
ylab = "Retention", xlab ="Credits FAILED", main = "Conditional density plot")
fit3 <- glm(IS_RETAINED ~ CLASS_LEVEL_STUDENT*CREDITS_FAILED,
data = mod2_df,
family = binomial(link = "logit"))
summary(fit3)
fit3a <- glm(IS_RETAINED ~ CLASS_LEVEL_STUDENT + CREDITS_FAILED,
data = mod2_df,
family = binomial(link = "logit"))
anova(fit3a, fit3, test = "LR")
fit3_emtrends <- emtrends(fit3, "CLASS_LEVEL_STUDENT",
var = "CREDITS_FAILED",
transform = "response")
fit3_emtrends
plot(fit3_emtrends)
mod2_df %>%
ggplot(aes(x = CREDIT_HOURS_EARNED,
y = CREDITS_FAILED)) +
geom_point()
fit4 <- glm(IS_RETAINED ~ CLASS_LEVEL_STUDENT*CREDITS_FAILED +
CLASS_LEVEL_STUDENT*CREDIT_HOURS_EARNED,
data = mod2_df,
family = "binomial")
anova(fit2, fit4, test = "LR")
anova(fit3, fit4, test = "LR")
anova(fit1, fit2, fit3, fit4)
knitr::opts_chunk$set(echo = TRUE)
cdplot(factor(IS_RETAINED) ~ CREDIT_HOURS_FAILED,
df,
col=c("lightgoldenrod", "lightcyan"),
ylab = "Retention", xlab ="Credits FAILED", main = "Conditional density plot")
knitr::opts_chunk$set(echo = TRUE)
# Install EDA related libraries
library(tidyverse)
library(ggplot2)
library(gridExtra)
#library(plyr)
library(dplyr) # for group by
#library(tidyr)
load("../../data/students_df.RData")
#df <- read.csv(file = "../../data/TEST_FILE_WEEK2_V2.csv")
dim(students_df)
str(students_df) # list of each variable
summary(students_df)
df = students_df
# convert variables to factor
df$RESIDENCY_CODE = factor(df$RESIDENCY_CODE)
df$CLASS_LEVEL_STUDENT = factor(df$CLASS_LEVEL_STUDENT)
df$MAJ_1_SCHOOL_NAME = factor(df$MAJ_1_SCHOOL_NAME)
df$STU_ORIG_ENROLL_STATUS = factor(df$STU_ORIG_ENROLL_STATUS)
df$FAC_IS_RETAINED = factor(df$IS_RETAINED)
# reorder levels of class
df$ORDER_CLASS_LEVEL_STUDENT = ordered(df$CLASS_LEVEL_STUDENT, levels = c('FR','SO','JU','SR'))
# check level
levels(df$ORDER_CLASS_LEVEL_STUDENT)
# check contrast
contrasts(df$ORDER_CLASS_LEVEL_STUDENT)
# log transform variables
df$LOG_TRNSFR_UNITS_ENTRY = log(df$TRNSFR_UNITS_ENTRY+0.1) # add a small number to make sure log is not inf
df$LOG_TEST_CREDIT_ENTRY = log(df$TEST_CREDIT_ENTRY+0.1)
# engineer new feature
df$CREDIT_HOURS_FAILED = df$CREDIT_HOURS_ATT - df$CREDIT_HOURS_EARNED
cdplot(factor(IS_RETAINED) ~ CREDIT_HOURS_FAILED,
df,
col=c("lightgoldenrod", "lightcyan"),
ylab = "Retention", xlab ="Credits FAILED", main = "Conditional density plot")
cdplot(factor(IS_RETAINED) ~ CREDIT_HOURS_ATT,
df,
col=c("lightgoldenrod", "lightcyan"),
ylab = "Retention", xlab ="Credits FAILED", main = "Conditional density plot")
cdplot(factor(IS_RETAINED) ~ CREDIT_HOURS_ATT,
df,
#col=c("lightgoldenrod", "lightcyan"),
ylab = "Retention", xlab ="Credits FAILED", main = "Conditional density plot")
cdplot(factor(IS_RETAINED) ~ CREDIT_HOURS_FAILED,
df,
#col=c("lightgoldenrod", "lightcyan"),
ylab = "Retention", xlab ="Credits FAILED", main = "Conditional density plot")
cdplot(factor(IS_RETAINED) ~ CREDIT_HOURS_ATT,
df,
#col=c("lightgoldenrod", "lightcyan"),
ylab = "Retention", xlab ="Credits ATTEMPTED", main = "Conditional density plot")
knitr::opts_chunk$set(echo = TRUE)
# Install EDA related libraries
library(tidyverse)
library(ggplot2)
library(gridExtra)
#library(plyr)
library(dplyr) # for group by
#library(tidyr)
load("../../data/students_df.RData")
#df <- read.csv(file = "../../data/TEST_FILE_WEEK2_V2.csv")
dim(students_df)
str(students_df) # list of each variable
summary(students_df)
df = students_df
# convert variables to factor
df$RESIDENCY_CODE = factor(df$RESIDENCY_CODE)
df$CLASS_LEVEL_STUDENT = factor(df$CLASS_LEVEL_STUDENT)
df$MAJ_1_SCHOOL_NAME = factor(df$MAJ_1_SCHOOL_NAME)
df$STU_ORIG_ENROLL_STATUS = factor(df$STU_ORIG_ENROLL_STATUS)
df$FAC_IS_RETAINED = factor(df$IS_RETAINED)
# reorder levels of class
df$ORDER_CLASS_LEVEL_STUDENT = ordered(df$CLASS_LEVEL_STUDENT, levels = c('FR','SO','JU','SR'))
# check level
levels(df$ORDER_CLASS_LEVEL_STUDENT)
# check contrast
contrasts(df$ORDER_CLASS_LEVEL_STUDENT)
# log transform variables
df$LOG_TRNSFR_UNITS_ENTRY = log(df$TRNSFR_UNITS_ENTRY+0.1) # add a small number to make sure log is not inf
df$LOG_TEST_CREDIT_ENTRY = log(df$TEST_CREDIT_ENTRY+0.1)
# engineer new feature
df$CREDIT_HOURS_FAILED = df$CREDIT_HOURS_ATT - df$CREDIT_HOURS_EARNED
# Partition the data
library(caTools)
set.seed(2020)
trainMask = sample.split(df$IS_RETAINED, SplitRatio = .75)
train = subset(df, trainMask == TRUE)
test  = subset(df, trainMask == FALSE)
# Model survival as a function of several predictors
ml_formula1 = formula(IS_RETAINED ~ RESIDENCY_CODE + ORDER_CLASS_LEVEL_STUDENT + IS_DBL_MAJOR + MAJ_1_SCHOOL_NAME + CUM_RESIDENT_TERMS_BOT + LOG_TRNSFR_UNITS_ENTRY + LOG_TEST_CREDIT_ENTRY + CREDIT_HOURS_FAILED + CREDIT_HOURS_ATT)
ml_formula2 = formula(IS_RETAINED ~ RESIDENCY_CODE + CLASS_LEVEL_STUDENT + IS_DBL_MAJOR + MAJ_1_SCHOOL_NAME + CUM_RESIDENT_TERMS_BOT + LOG_TRNSFR_UNITS_ENTRY + LOG_TEST_CREDIT_ENTRY + CREDIT_HOURS_FAILED + CREDIT_HOURS_ATT)
# Train a logistic regression model
ml_log1 = glm(ml_formula1, family=binomial, data = train)
ml_log2 = glm(ml_formula2, family=binomial, data = train)
# Summarize the model
summary(ml_log1)
summary(ml_log2)
#confint(ml_log2) # to get confidence interval
exp(1.6)
exp(0.6)
#Predict using the model and find best cutoff
x = subset(test,selext=c("IS_RETAINED","IS_RETAINED_DESC"))
y = test$IS_RETAINED
test$prob_lreg = predict(ml_log2,x,type="response")
library(InformationValue)
optCutOff = optimalCutoff(test$IS_RETAINED, test$prob_lreg)[1]
optCutOff #=0.63
test$pred_lreg = as.numeric(test$prob_lreg >= optCutOff)
sum(test$pred_lreg) # =3535
dim(test) # 3560 x 43
misClassError(test$IS_RETAINED, test$lreg_prob, threshold = optCutOff)
# 0.0295
#Accuracy of the model
mtab = table(test$IS_RETAINED,test$pred_lreg)
library(caret)
confusionMatrix(mtab)
plotROC(test$IS_RETAINED, test$prob_lreg)
library(MASS)
#Build the model
ml_lda<-lda(ml_formula2,data=train)
#Summarize the model
summary(ml_lda)
#Predict using the model
test$pred_lda = predict(ml_lda,x)$class
#Accuracy of the model
mtab = table(test$IS_RETAINED,test$pred_lda)
confusionMatrix(mtab)
library(mda)
#Build the model
ml_mda = mda(ml_formula2,data=train)
#Summarize the model
summary(ml_mda)
#Predict using the model
test$pred_mda = predict(ml_mda,x)
#Accuracy of the model
mtab = table(test$IS_RETAINED,test$pred_mda)
confusionMatrix(mtab)
library(kernlab)
ml_svm = ksvm(ml_formula2,data=train)
#Summarize the model
summary(ml_svm)
#Predict using the model
test$prob_svm = predict(ml_svm,x,type='response')
optCutOff = optimalCutoff(test$IS_RETAINED, test$prob_svm)[1]
optCutOff # 0.86
test$pred_svm = as.numeric(test$prob_svm>=optCutOff)
#Accuracy of the model
mtab = table(test$IS_RETAINED,test$pred_svm)
confusionMatrix(mtab)
plotROC(test$IS_RETAINED, test$prob_svm)
library(rpart)
library(rpart.plot)
#Build the model
# For classification tree, response needs to be factor
# tree doesn't need log transform
ml_formula3 = formula(FAC_IS_RETAINED ~ RESIDENCY_CODE + CLASS_LEVEL_STUDENT + IS_DBL_MAJOR + MAJ_1_SCHOOL_NAME + CUM_RESIDENT_TERMS_BOT + TRNSFR_UNITS_ENTRY + TEST_CREDIT_ENTRY + CREDIT_HOURS_FAILED + CREDIT_HOURS_ATT)
ml_tree = rpart(ml_formula3,data=df,cp=0.01) # cp for complexity, smaller cp =more branches
#Summarize the model
summary(ml_tree)
# Visualize the decision tree with rpart.plot
rpart.plot(ml_tree, box.palette="RdBu", shadow.col="gray", nn=TRUE)
#Predict using the model
test$pred_tree = predict(ml_tree,x,type="class")
#Accuracy of the model
mtab = table(test$IS_RETAINED,test$pred_tree)
confusionMatrix(mtab)
library(randomForest)
#Build the model (need to make sure no chr variables, convert to factors)
ml_rf = randomForest(ml_formula3,data=df,localImp = TRUE) # add local importance interpretation for later
#Summarize the model
summary(ml_rf)
#getTree(ml_rf, 1) # check an example tree
importance(ml_rf) # list of feature importance
# Takes too long, only run if necessary
# library(randomForestExplainer)
# #setwd(my/destination/path)
# explain_forest(ml_rf, interactions = TRUE, data = train)
# # This will generate an .html file, named Your_forest_explained.html, in your my/destination/path that you can easily open in a Web Browser.
# # In this report you'll find useful information about the structure of trees and forest and several useful statistics about the variables.
#Predict using the model
xtest = cbind(train[1,],x) # to equalize classes of training and test set, otherwise throw error
test$pred_rf = predict(ml_rf,xtest)
#Accuracy of the model
mtab = table(test$IS_RETAINED,test$pred_rf)
confusionMatrix(mtab)
library(gbm)
#Build the model
ml_gbm = gbm(ml_formula3,data=train,distribution="multinomial") # binomial is not supported (why?)
#Summarize the model
summary(ml_gbm)
#Predict using the model
test$prob_gbm = predict(ml_gbm,x,n.trees=1)
optCutOff = optimalCutoff(test$IS_RETAINED, test$prob_gbm)[1]
optCutOff # -0.18 something is wrong...
test$pred_gbm = as.numeric(test$prob_gbm>=optCutOff)
df$CREDIT_HOURS_FAILPER = df$CREDIT_HOURS_FAILED / df$CREDIT_HOURS_ATT
cdplot(factor(IS_RETAINED) ~ CREDIT_HOURS_FAILPER,
df,
#col=c("lightgoldenrod", "lightcyan"),
ylab = "Retention", xlab ="Credits FAILED PERCENT", main = "Conditional density plot")
library(rpart)
library(rpart.plot)
#Build the model
# For classification tree, response needs to be factor
# tree doesn't need log transform
ml_formula3 = formula(FAC_IS_RETAINED ~ RESIDENCY_CODE + CLASS_LEVEL_STUDENT + IS_DBL_MAJOR + MAJ_1_SCHOOL_NAME + CUM_RESIDENT_TERMS_BOT + TRNSFR_UNITS_ENTRY + TEST_CREDIT_ENTRY + CREDIT_HOURS_FAILED + CREDIT_HOURS_FAILPER)
ml_tree = rpart(ml_formula3,data=df,cp=0.01) # cp for complexity, smaller cp =more branches
#Summarize the model
summary(ml_tree)
# Visualize the decision tree with rpart.plot
rpart.plot(ml_tree, box.palette="RdBu", shadow.col="gray", nn=TRUE)
library(rpart)
library(rpart.plot)
#Build the model
# For classification tree, response needs to be factor
# tree doesn't need log transform
ml_formula3 = formula(FAC_IS_RETAINED ~ RESIDENCY_CODE + CLASS_LEVEL_STUDENT + IS_DBL_MAJOR + MAJ_1_SCHOOL_NAME + CUM_RESIDENT_TERMS_BOT + TRNSFR_UNITS_ENTRY + TEST_CREDIT_ENTRY + CREDIT_HOURS_FAILED + CREDIT_HOURS_FAILPER)
ml_tree = rpart(ml_formula3,data=df,cp=0.001) # cp for complexity, smaller cp =more branches
#Summarize the model
summary(ml_tree)
# Visualize the decision tree with rpart.plot
rpart.plot(ml_tree, box.palette="RdBu", shadow.col="gray", nn=TRUE)
library(rpart)
library(rpart.plot)
#Build the model
# For classification tree, response needs to be factor
# tree doesn't need log transform
ml_formula3 = formula(FAC_IS_RETAINED ~ RESIDENCY_CODE + CLASS_LEVEL_STUDENT + IS_DBL_MAJOR + MAJ_1_SCHOOL_NAME + CUM_RESIDENT_TERMS_BOT + TRNSFR_UNITS_ENTRY + TEST_CREDIT_ENTRY + CREDIT_HOURS_FAILED + CREDIT_HOURS_FAILPER)
ml_tree = rpart(ml_formula3,data=df,cp=0.05) # cp for complexity, smaller cp =more branches
#Summarize the model
summary(ml_tree)
# Visualize the decision tree with rpart.plot
rpart.plot(ml_tree, box.palette="RdBu", shadow.col="gray", nn=TRUE)
library(rpart)
library(rpart.plot)
#Build the model
# For classification tree, response needs to be factor
# tree doesn't need log transform
ml_formula3 = formula(FAC_IS_RETAINED ~ RESIDENCY_CODE + CLASS_LEVEL_STUDENT + IS_DBL_MAJOR + MAJ_1_SCHOOL_NAME + CUM_RESIDENT_TERMS_BOT + TRNSFR_UNITS_ENTRY + TEST_CREDIT_ENTRY + CREDIT_HOURS_FAILED + CREDIT_HOURS_FAILPER)
ml_tree = rpart(ml_formula3,data=df,cp=0.02) # cp for complexity, smaller cp =more branches
#Summarize the model
summary(ml_tree)
# Visualize the decision tree with rpart.plot
rpart.plot(ml_tree, box.palette="RdBu", shadow.col="gray", nn=TRUE)
library(rpart)
library(rpart.plot)
#Build the model
# For classification tree, response needs to be factor
# tree doesn't need log transform
ml_formula3 = formula(FAC_IS_RETAINED ~ RESIDENCY_CODE + CLASS_LEVEL_STUDENT + IS_DBL_MAJOR + MAJ_1_SCHOOL_NAME + CUM_RESIDENT_TERMS_BOT + TRNSFR_UNITS_ENTRY + TEST_CREDIT_ENTRY + CREDIT_HOURS_FAILED + CREDIT_HOURS_FAILPER)
ml_tree = rpart(ml_formula3,data=df,cp=0.005) # cp for complexity, smaller cp =more branches
#Summarize the model
summary(ml_tree)
# Visualize the decision tree with rpart.plot
rpart.plot(ml_tree, box.palette="RdBu", shadow.col="gray", nn=TRUE)
library(rpart)
library(rpart.plot)
#Build the model
# For classification tree, response needs to be factor
# tree doesn't need log transform
ml_formula3 = formula(FAC_IS_RETAINED ~ RESIDENCY_CODE + CLASS_LEVEL_STUDENT + IS_DBL_MAJOR + MAJ_1_SCHOOL_NAME + CUM_RESIDENT_TERMS_BOT + TRNSFR_UNITS_ENTRY + TEST_CREDIT_ENTRY + CREDIT_HOURS_FAILED + CREDIT_HOURS_FAILPER)
ml_tree = rpart(ml_formula3,data=df,cp=0.008) # cp for complexity, smaller cp =more branches
#Summarize the model
summary(ml_tree)
# Visualize the decision tree with rpart.plot
rpart.plot(ml_tree, box.palette="RdBu", shadow.col="gray", nn=TRUE)
library(rpart)
library(rpart.plot)
#Build the model
# For classification tree, response needs to be factor
# tree doesn't need log transform
ml_formula3 = formula(FAC_IS_RETAINED ~ RESIDENCY_CODE + CLASS_LEVEL_STUDENT + IS_DBL_MAJOR + MAJ_1_SCHOOL_NAME + CUM_RESIDENT_TERMS_BOT + TRNSFR_UNITS_ENTRY + TEST_CREDIT_ENTRY + CREDIT_HOURS_FAILED + CREDIT_HOURS_FAILPER)
ml_tree = rpart(ml_formula3,data=df,cp=0.007) # cp for complexity, smaller cp =more branches
#Summarize the model
summary(ml_tree)
# Visualize the decision tree with rpart.plot
rpart.plot(ml_tree, box.palette="RdBu", shadow.col="gray", nn=TRUE)
library(rpart)
library(rpart.plot)
#Build the model
# For classification tree, response needs to be factor
# tree doesn't need log transform
ml_formula3 = formula(FAC_IS_RETAINED ~ RESIDENCY_CODE + CLASS_LEVEL_STUDENT + IS_DBL_MAJOR + MAJ_1_SCHOOL_NAME + CUM_RESIDENT_TERMS_BOT + TRNSFR_UNITS_ENTRY + TEST_CREDIT_ENTRY + CREDIT_HOURS_FAILED + CREDIT_HOURS_FAILPER)
ml_tree = rpart(ml_formula3,data=df,cp=0.006) # cp for complexity, smaller cp =more branches
#Summarize the model
summary(ml_tree)
# Visualize the decision tree with rpart.plot
rpart.plot(ml_tree, box.palette="RdBu", shadow.col="gray", nn=TRUE)
ml_tree = rpart(ml_formula3,data=df,cp=0.005) # cp for complexity, smaller cp =more branches
# Visualize the decision tree with rpart.plot
rpart.plot(ml_tree, box.palette="RdBu", shadow.col="gray", nn=TRUE)
library(rpart)
library(rpart.plot)
#Build the model
# For classification tree, response needs to be factor
# tree doesn't need log transform
ml_formula3 = formula(FAC_IS_RETAINED ~ RESIDENCY_CODE + CLASS_LEVEL_STUDENT + IS_DBL_MAJOR + MAJ_1_SCHOOL_NAME + CUM_RESIDENT_TERMS_BOT + TRNSFR_UNITS_ENTRY + TEST_CREDIT_ENTRY + CREDIT_HOURS_FAILED + CREDIT_HOURS_EARNED)
ml_tree = rpart(ml_formula3,data=df,cp=0.01) # cp for complexity, smaller cp =more branches
#Summarize the model
summary(ml_tree)
# Visualize the decision tree with rpart.plot
rpart.plot(ml_tree, box.palette="RdBu", shadow.col="gray", nn=TRUE)
ml_tree = rpart(ml_formula3,data=df,cp=0.008) # cp for complexity, smaller cp =more branches
library(rpart)
library(rpart.plot)
#Build the model
# For classification tree, response needs to be factor
# tree doesn't need log transform
ml_formula3 = formula(FAC_IS_RETAINED ~ RESIDENCY_CODE + CLASS_LEVEL_STUDENT + IS_DBL_MAJOR + MAJ_1_SCHOOL_NAME + CUM_RESIDENT_TERMS_BOT + TRNSFR_UNITS_ENTRY + TEST_CREDIT_ENTRY + CREDIT_HOURS_FAILED + CREDIT_HOURS_EARNED)
ml_tree = rpart(ml_formula3,data=df,cp=0.008) # cp for complexity, smaller cp =more branches
#Summarize the model
#summary(ml_tree)
# Visualize the decision tree with rpart.plot
rpart.plot(ml_tree, box.palette="RdBu", shadow.col="gray", nn=TRUE)
cdplot(factor(IS_RETAINED) ~ CREDIT_HOURS_EARNED,
df,
#col=c("lightgoldenrod", "lightcyan"),
ylab = "Retention", xlab ="Credits EARNED", main = "Conditional density plot")
glm1 <- glm(IS_RETAINED ~ FAILED*EARNED,
data = df,
family = binomial(link = "logit"))
glm1 <- glm(IS_RETAINED ~ FAILED*EARNED,
data = df,
family = binomial(link = "logit"))
# convert variables to factor
df$FAILED = cut(
df$CREDIT_HOURS_FAILED,
breaks = c(0, 5, 13, Inf),
labels = c("low_<=5", "med_5<=13", "high_13<=Inf"),
right  = TRUE # include right hand number
)
df$EARNED = cut(
df$CREDIT_HOURS_EARNED,
breaks = c(0, 4, 7, Inf),
labels = c("low_<=4", "med_4<=7", "high_7<=Inf"),
right  = TRUE # include right hand number
)
glm1 <- glm(IS_RETAINED ~ FAILED*EARNED,
data = df,
family = binomial(link = "logit"))
summary(glm1)
glm1 <- glm(IS_RETAINED ~ FAILED*CLASS_LEVEL_STUDENT*IS_DOUBLE_MAJOR,
data = df,
family = binomial(link = "logit"))
glm1 <- glm(IS_RETAINED ~ FAILED*CLASS_LEVEL_STUDENT*IS_DBL_MAJOR,
data = df,
family = binomial(link = "logit"))
summary(glm1)
glm2 <- glm(IS_RETAINED ~ FAILED*CLASS_LEVEL_STUDENT*IS_DBL_MAJOR,
data = df,
family = binomial(link = "logit"))
summary(glm2)
glm3 <- glm(IS_RETAINED ~ FAILED*CLASS_LEVEL_STUDENT,
data = df,
family = binomial(link = "logit"))
summary(glm3)
fit3_emtrends <- emtrends(fit3, "CLASS_LEVEL_STUDENT",
var = "FAILED",
transform = "response")
glm3_emtrends <- emtrends(glm3, "CLASS_LEVEL_STUDENT",
var = "FAILED",
transform = "response")
# Libraries
library(forcats)
library(hrbrthemes)
# Libraries
#library(forcats)
#library(hrbrthemes)
#library(viridis)
# Grouped
df %>%
ggplot(aes(fill=FAILED, y=IS_RETAINED, x=CLASS_LEVEL_STUDENT)) +
geom_violin(position="dodge", alpha=0.5, outlier.colour="transparent") +
scale_fill_viridis(discrete=T, name="") +
theme_ipsum()  +
xlab("") +
ylab("Tip (%)") +
ylim(0,40)
# Libraries
#library(forcats)
#library(hrbrthemes)
#library(viridis)
# Grouped
df %>%
ggplot(aes(fill=FAILED, y=IS_RETAINED, x=CLASS_LEVEL_STUDENT)) +
geom_violin(position="dodge", alpha=0.5, outlier.colour="transparent") +
xlab("") +
ylab("Tip (%)") +
ylim(0,40)
# Libraries
library(forcats)
library(hrbrthemes)
install.packages("viridis")
# Libraries
library(forcats)
#library(hrbrthemes)
library(viridis)
# Grouped
df %>%
ggplot(aes(fill=FAILED, y=IS_RETAINED, x=CLASS_LEVEL_STUDENT)) +
geom_violin(position="dodge", alpha=0.5, outlier.colour="transparent",addDot=TRUE, dotSize=1.7, dotPosition="jitter", jitter=0.2) +
scale_fill_viridis(discrete=T, name="") +
theme_ipsum()  +
xlab("") +
ylab("Tip (%)") +
ylim(0,40)
# Libraries
library(forcats)
#library(hrbrthemes)
library(viridis)
# Grouped
df %>%
ggplot(aes(fill=FAILED, y=IS_RETAINED, x=CLASS_LEVEL_STUDENT)) +
geom_violin(position="dodge", alpha=0.5, outlier.colour="transparent",addDot=TRUE, dotSize=1.7, dotPosition="jitter", jitter=0.2) +
scale_fill_viridis(discrete=T, name="") +
xlab("") +
ylab("Tip (%)") +
ylim(0,40)
# Libraries
library(forcats)
#library(hrbrthemes)
library(viridis)
# Grouped
df %>%
ggplot(aes(fill=FAILED, y=IS_RETAINED, x=CLASS_LEVEL_STUDENT)) +
geom_violin(position="dodge", alpha=0.5, outlier.colour="transparent",addDot=TRUE, dotSize=1.7) +
scale_fill_viridis(discrete=T, name="") +
xlab("") +
ylab("Tip (%)") +
ylim(0,40)
ggplot2.violinplot(data=df, xName='CLASS_LEVEL_STUDENT',yName='IS_RETAINED',
groupName='FAILED',
groupColors=c('#999999','#E69F00','#56B4E9'), showLegend=FALSE,
backgroundColor="white", xtitle="Dose (mg)", ytitle="length",
mainTitle="Plot of length \n by dose",
addDot=TRUE, dotSize=1)
# Libraries
install.packages("devtools")
library(devtools)
install_github("easyGgplot2", "kassambara")
install.packages("devtools")
knitr::opts_chunk$set(echo = TRUE)
install_github("easyGgplot2", "kassambara")
# Libraries
install.packages("devtools")
# Libraries
install.packages("devtools")
library(devtools)
install_github("easyGgplot2", "kassambara")
ggplot2.violinplot(data=df, xName='CLASS_LEVEL_STUDENT',yName='IS_RETAINED',
groupName='FAILED',
groupColors=c('#999999','#E69F00','#56B4E9'), showLegend=FALSE,
backgroundColor="white", xtitle="Dose (mg)", ytitle="length",
mainTitle="Plot of length \n by dose",
addDot=TRUE, dotSize=1)
library(easyGgplot2)
install_github("easyGgplot2", "kassambara")
install_github("easyGgplot2", "kassambara")
library(easyGgplot2)
