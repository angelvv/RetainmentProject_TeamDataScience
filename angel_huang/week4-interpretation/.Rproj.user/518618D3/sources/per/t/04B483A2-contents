---
title: "04_plot"
author: "Angel Huang"
date: "11/10/2020"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# 1 Prepare data
## Load libraries
```{r}
# Install EDA related libraries
library(tidyverse)
library(ggplot2)
library(gridExtra)
#library(plyr)
library(dplyr) # for group by
#library(tidyr)
```

## Load data from rdata that was generated by 01_make_data in data folder
```{r}
load("../../data/students_df.RData")
#df <- read.csv(file = "../../data/TEST_FILE_WEEK2_V2.csv")
dim(students_df)
str(students_df) # list of each variable
summary(students_df)
df = students_df
```


## Transformation of variables
```{r}
# convert variables to factor
df$RESIDENCY_CODE = factor(df$RESIDENCY_CODE)
df$CLASS_LEVEL_STUDENT = factor(df$CLASS_LEVEL_STUDENT)
df$MAJ_1_SCHOOL_NAME = factor(df$MAJ_1_SCHOOL_NAME)
df$STU_ORIG_ENROLL_STATUS = factor(df$STU_ORIG_ENROLL_STATUS)
df$FAC_IS_RETAINED = factor(df$IS_RETAINED)

# reorder levels of class
df$ORDER_CLASS_LEVEL_STUDENT = ordered(df$CLASS_LEVEL_STUDENT, levels = c('FR','SO','JU','SR'))
# check level
levels(df$ORDER_CLASS_LEVEL_STUDENT)
# check contrast
contrasts(df$ORDER_CLASS_LEVEL_STUDENT)

# log transform variables
df$LOG_TRNSFR_UNITS_ENTRY = log(df$TRNSFR_UNITS_ENTRY+0.1) # add a small number to make sure log is not inf
df$LOG_TEST_CREDIT_ENTRY = log(df$TEST_CREDIT_ENTRY+0.1)
# engineer new feature
df$CREDIT_HOURS_FAILED = df$CREDIT_HOURS_ATT - df$CREDIT_HOURS_EARNED
df$CREDIT_HOURS_FAILPER = df$CREDIT_HOURS_FAILED / df$CREDIT_HOURS_ATT
```
I reordered the class level so that it makes more sense -- from low to high grade. Since some variables are skewed, I created log transform of those variables for linear models later.

## Variable engineering: From previous 03_tree models, we discovered that credits failed and attempted are the first two main split, based on those split, we will convert failed credits into categories.

### Classification and Regression Tree (CART)
```{r train}
library(rpart)
library(rpart.plot)
#Build the model
# For classification tree, response needs to be factor
# tree doesn't need log transform
ml_formula3 = formula(FAC_IS_RETAINED ~ RESIDENCY_CODE + CLASS_LEVEL_STUDENT + IS_DBL_MAJOR + MAJ_1_SCHOOL_NAME + CUM_RESIDENT_TERMS_BOT + TRNSFR_UNITS_ENTRY + TEST_CREDIT_ENTRY + CREDIT_HOURS_FAILED + CREDIT_HOURS_EARNED)

ml_tree = rpart(ml_formula3,data=df,cp=0.008) # cp for complexity, smaller cp =more branches

#Summarize the model
#summary(ml_tree)
# Visualize the decision tree with rpart.plot
rpart.plot(ml_tree, box.palette="RdBu", shadow.col="gray", nn=TRUE)
```


First, we do a density plot to visually verify the split
```{r}
cdplot(factor(IS_RETAINED) ~ CREDIT_HOURS_FAILED, 
       df,
       #col=c("lightgoldenrod", "lightcyan"), 
       ylab = "Retention", xlab ="Credits FAILED", main = "Conditional density plot")
```
More failed credits, more dropout. The split happens at <=5, <13, consistent with the tree split.

Then density plot on credits attempted.
```{r}
cdplot(factor(IS_RETAINED) ~ CREDIT_HOURS_EARNED, 
       df,
       #col=c("lightgoldenrod", "lightcyan"), 
       ylab = "Retention", xlab ="Credits EARNED", main = "Conditional density plot")
```
The split happens at <4, <7.3, consistent with the tree split.

## Create new variables
```{r}
# convert variables to factor
df$FAILED = cut(
  df$CREDIT_HOURS_FAILED,
  breaks = c(0, 5, 13, Inf),
  labels = c("low_<=5", "med_5<=13", "high_13<=Inf"),
  right  = TRUE # include right hand number
)
df$EARNED = cut(
  df$CREDIT_HOURS_EARNED,
  breaks = c(0, 4, 7, Inf),
  labels = c("low_<=4", "med_4<=7", "high_7<=Inf"),
  right  = TRUE # include right hand number
)
```

# 3. GLM model
```{r}
glm1 <- glm(IS_RETAINED ~ FAILED*EARNED, 
            data = df,
            family = binomial(link = "logit"))
summary(glm1)
```
There is only one interaction is sig, which is failed 5~13 and earned more than 7 credits, which significantly reduces the probability of return.
  
```{r}
glm2 <- glm(IS_RETAINED ~ FAILED*CLASS_LEVEL_STUDENT*IS_DBL_MAJOR, 
            data = df,
            family = binomial(link = "logit"))
summary(glm2)
glm3 <- glm(IS_RETAINED ~ FAILED*CLASS_LEVEL_STUDENT, 
            data = df,
            family = binomial(link = "logit"))
summary(glm3)



```
Only failed * class_level interaction is significant.

# plot
```{r}
# Libraries
install.packages("devtools")
library(devtools)
install_github("easyGgplot2", "kassambara")
library(easyGgplot2)

ggplot2.violinplot(data=df, xName='CLASS_LEVEL_STUDENT',yName='IS_RETAINED', 
    groupName='FAILED',
    groupColors=c('#999999','#E69F00','#56B4E9'), showLegend=TRUE,
    backgroundColor="white", xtitle="Class level", ytitle="Is retained", 
    mainTitle="Interaction effect of failed credit and class level on is_retained",
    addDot=TRUE, dotSize=1)
```
Since in decision tree model, we have observed critical split of variable "FAILED_CREDIT", and 

```{r, message=F}
glm3_emmeans <- emmeans(glm3, CLASS_LEVEL_STUDENT,
                        type = "response") %>%
  as.data.frame
class_eff_df
```

# For category and continuous variable
```{r}
glm3_emtrends <- emtrends(glm3, "CLASS_LEVEL_STUDENT", 
         var = "FAILED",
         transform = "response")
glm3_emtrends
```


  