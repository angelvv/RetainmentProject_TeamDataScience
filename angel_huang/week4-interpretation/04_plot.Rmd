---
title: "04_plot"
author: "Angel Huang"
date: "11/10/2020"
output:
  html_document:
    df_print: paged
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# 1 Prepare data
## Load libraries
```{r}
# Install EDA related libraries
library(tidyverse)
library(ggplot2)
library(gridExtra)
#library(plyr)
library(dplyr) # for group by
#library(tidyr)
```

## Load data from rdata that was generated by 01_make_data in data folder
```{r}
load("../../data/students_df.RData")
#df <- read.csv(file = "../../data/TEST_FILE_WEEK2_V2.csv")
dim(students_df)
str(students_df) # list of each variable
summary(students_df)
df = students_df
```


## Transformation of variables
```{r}
# convert variables to factor
df$RESIDENCY_CODE = factor(df$RESIDENCY_CODE)
df$CLASS_LEVEL_STUDENT = factor(df$CLASS_LEVEL_STUDENT)
df$MAJ_1_SCHOOL_NAME = factor(df$MAJ_1_SCHOOL_NAME)
df$STU_ORIG_ENROLL_STATUS = factor(df$STU_ORIG_ENROLL_STATUS)
df$FAC_IS_RETAINED = factor(df$IS_RETAINED)

# reorder levels of class
df$ORDER_CLASS_LEVEL_STUDENT = ordered(df$CLASS_LEVEL_STUDENT, levels = c('FR','SO','JU','SR'))
# check level
levels(df$ORDER_CLASS_LEVEL_STUDENT)
# check contrast
contrasts(df$ORDER_CLASS_LEVEL_STUDENT)

# log transform variables
df$LOG_TRNSFR_UNITS_ENTRY = log(df$TRNSFR_UNITS_ENTRY+0.1) # add a small number to make sure log is not inf
df$LOG_TEST_CREDIT_ENTRY = log(df$TEST_CREDIT_ENTRY+0.1)
# engineer new feature
df$CREDIT_HOURS_FAILED = df$CREDIT_HOURS_ATT - df$CREDIT_HOURS_EARNED
df$CREDIT_HOURS_FAILPER = df$CREDIT_HOURS_FAILED / df$CREDIT_HOURS_ATT
```
I reordered the class level so that it makes more sense -- from low to high grade. Since some variables are skewed, I created log transform of those variables for linear models later.

## Variable engineering: From previous 03_tree models, we discovered that credits failed and attempted are the first two main split, based on those split, we will convert failed credits into categories.

### Classification and Regression Tree (CART)
```{r train}
library(rpart)
library(rpart.plot)
#Build the model
# For classification tree, response needs to be factor
# tree doesn't need log transform
ml_formula3 = formula(FAC_IS_RETAINED ~ RESIDENCY_CODE + CLASS_LEVEL_STUDENT + IS_DBL_MAJOR + MAJ_1_SCHOOL_NAME + CUM_RESIDENT_TERMS_BOT + TRNSFR_UNITS_ENTRY + TEST_CREDIT_ENTRY + CREDIT_HOURS_FAILED + CREDIT_HOURS_EARNED)

ml_tree = rpart(ml_formula3,data=df,cp=0.008) # cp for complexity, smaller cp =more branches

#Summarize the model
#summary(ml_tree)
# Visualize the decision tree with rpart.plot
rpart.plot(ml_tree, box.palette="RdBu", shadow.col="gray", nn=TRUE)
```


First, we do a density plot to visually verify the split
```{r}
cdplot(factor(IS_RETAINED) ~ CREDIT_HOURS_FAILED, 
       df,
       #col=c("lightgoldenrod", "lightcyan"), 
       ylab = "Retention", xlab ="Credits FAILED", main = "Conditional density plot")
```
More failed credits, more dropout. The split happens at <=5, <13, consistent with the tree split.

Then density plot on credits attempted.
```{r}
cdplot(factor(IS_RETAINED) ~ CREDIT_HOURS_EARNED, 
       df,
       #col=c("lightgoldenrod", "lightcyan"), 
       ylab = "Retention", xlab ="Credits EARNED", main = "Conditional density plot")
```
The split happens at <4, <7.3, consistent with the tree split.

## Create new variables
```{r}
# convert variables to factor
df$FAILED = cut(
  df$CREDIT_HOURS_FAILED,
  breaks = c(-Inf, 5, 13, Inf), # If use 0 for the first one will make all entries with 0 into NA
  labels = c("low_<=5", "med_5<=13", "high_13<=Inf"),
  right  = TRUE # include right hand number
)
df$EARNED = cut(
  df$CREDIT_HOURS_EARNED,
  breaks = c(-Inf, 4, 7, Inf),
  labels = c("low_<=4", "med_4<=7", "high_7<=Inf"),
  right  = TRUE # include right hand number
)
```

# 3. GLM model
```{r}
glm1 <- glm(IS_RETAINED ~ FAILED*EARNED, 
            data = df,
            family = binomial(link = "logit"))
summary(glm1)
```
There is only one interaction is sig, which is failed 5~13 and earned more than 7 credits, which significantly reduces the probability of return.
  
```{r}
#glm2 <- glm(IS_RETAINED ~ FAILED*CLASS_LEVEL_STUDENT*IS_DBL_MAJOR, 
#            data = df,
#            family = binomial(link = "logit"))
#summary(glm2)
glm3 <- glm(IS_RETAINED ~ FAILED*CLASS_LEVEL_STUDENT, 
            data = df,
            family = binomial(link = "logit"))
summary(glm3)

ml_formula1 = formula(IS_RETAINED ~ RESIDENCY_CODE + IS_DBL_MAJOR + MAJ_1_SCHOOL_NAME + CUM_RESIDENT_TERMS_BOT + LOG_TRNSFR_UNITS_ENTRY + LOG_TEST_CREDIT_ENTRY + FAILED*CLASS_LEVEL_STUDENT + CREDIT_HOURS_ATT)

glm4 = glm(ml_formula1,data=df,family = binomial(link = "logit"))
#summary(glm4)
sub_df = df[,c("RESIDENCY_CODE", "IS_DBL_MAJOR", "MAJ_1_SCHOOL_NAME","CUM_RESIDENT_TERMS_BOT", "LOG_TRNSFR_UNITS_ENTRY","LOG_TEST_CREDIT_ENTRY","FAILED","CLASS_LEVEL_STUDENT", "CREDIT_HOURS_ATT")]
df$pred = predict(glm4,sub_df,type="response")
```
Only failed * class_level interaction is significant.

# plot probability of retained by class level and failed
```{r}
ggplot(data=df, aes(x=CLASS_LEVEL_STUDENT, y=pred, fill=FAILED)) +
  geom_boxplot() + 
  ylab("GLM estimated probability of return") +
  xlab("Student class level") + 
  labs(fill = "Failed credits")
  #geom_dotplot(binaxis='y', stackdir='center',dotsize=0.1) # 
  

```
Using a decision tree model, we have observed critical splits of variable "FAILED_CREDIT", based on this, we divided this variable into 3 intervals: low=0-5, med=5.5-13, high=13.5-Inf for easier interpretation. In GLM model, we found significant interaction between this categorical failed credit variable with student class level. Here we use a box plot to visualize this interaction effect of failed credits and student class level on (GLM estimated) probability of return: when the failed credits are low (<=5), all grades have high probability of return (eg. red boxes at around 1). When failed credits are 5 to 13, junior and senior are more likely to return, freshman is the least likely to return with a probability of around 70%. When failed credits are above 13, the probability of return for freshman and junior dropped below 50%, which is pretty bad. In general, the slope (sensitivity) of the effect of failed credit on returning probability is decreasing as grade goes up, i.e. more senior students are more likely to return given failed credits.


```{r, message=F}
#glm3_emmeans <- emmeans(glm3, CLASS_LEVEL_STUDENT,
#                        type = "response") %>%
#  as.data.frame
#class_eff_df
```

# For category and continuous variable
```{r}
#glm3_emtrends <- emtrends(glm3, "CLASS_LEVEL_STUDENT", 
#         var = "FAILED",
#         transform = "response")
#glm3_emtrends
```


  